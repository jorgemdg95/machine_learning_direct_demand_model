{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e241f1b4",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d98c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import jaccard_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef449e",
   "metadata": {},
   "source": [
    "# Reading database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a09a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define file paths for the datasets\n",
    "file_path_training = os.path.join(current_directory, 'training_dataset.csv')\n",
    "file_path_prediction = os.path.join(current_directory, 'prediction_dataset.csv')\n",
    "\n",
    "# Read training dataset\n",
    "df_training = pd.read_csv(file_path_training)\n",
    "\n",
    "# Remove the 'Cluster' column from the training dataset\n",
    "df_training.drop('Cluster', axis=1, inplace=True)\n",
    "\n",
    "# Read prediction dataset\n",
    "df_prediction = pd.read_csv(file_path_prediction)\n",
    "\n",
    "\n",
    "df = pd.concat([df_training, df_prediction], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Record the starting index of the prediction dataset rows\n",
    "prediction_start_index = len(df_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a601ee",
   "metadata": {},
   "source": [
    "# Defining cluster variables and scaling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['spacing', 'interconnectivity', 'design', 'directness', 'service_area']].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01a501",
   "metadata": {},
   "source": [
    "# Clustering and deriving performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimum parameters \n",
    "n_clusters = 5  \n",
    "m = 1.3  \n",
    "\n",
    "\n",
    "\n",
    "def kmeans_plusplus_initializer(X, n_clusters):\n",
    "    n_samples, n_features = X.shape\n",
    "    centroids = np.zeros((n_clusters, n_features))\n",
    "    # Randomly choose the first centroid from the data points\n",
    "    initial_centroid_index = np.random.randint(n_samples)\n",
    "    centroids[0] = X[initial_centroid_index]\n",
    "\n",
    "    # Compute distances from the first centroid chosen to all the other data points\n",
    "    distances = np.linalg.norm(X - centroids[0], axis=1)\n",
    "    \n",
    "    for i in range(1, n_clusters):\n",
    "        # Choose next centroid with probability proportional to the square of the distance\n",
    "        probabilities = distances**2\n",
    "        probabilities /= probabilities.sum()\n",
    "        centroid_index = np.random.choice(n_samples, p=probabilities)\n",
    "        centroids[i] = X[centroid_index]\n",
    "\n",
    "        # Update distances after adding the new centroid\n",
    "        new_distances = np.linalg.norm(X - centroids[i], axis=1)\n",
    "        distances = np.minimum(distances, new_distances)\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "# Initialize centroids using k-means++\n",
    "initial_centroids = kmeans_plusplus_initializer(X_scaled, n_clusters)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# Configure FCM with the initialized centroids\n",
    "fcm = FCM(n_clusters=n_clusters, m=m)\n",
    "fcm.fit(X_scaled)\n",
    "\n",
    "# Get the resulting labels and centers\n",
    "labels = fcm.u.argmax(axis=1)\n",
    "centers = fcm.centers\n",
    "\n",
    "# Evaluation metrics\n",
    "silhouette = silhouette_score(X_scaled, labels)\n",
    "db_index = davies_bouldin_score(X_scaled, labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled, labels)\n",
    "\n",
    "# Calculate Average Shortest Euclidean Distance between centers\n",
    "distances = np.linalg.norm(centers[:, np.newaxis] - centers, axis=2)\n",
    "np.fill_diagonal(distances, np.inf)\n",
    "avg_shortest_distance = np.min(distances)\n",
    "\n",
    "# Stability using Adjusted Rand Index\n",
    "# Note: Ensure you have a previous run or a baseline to compare with, or simulate another run here\n",
    "previous_labels = np.copy(labels)  # Simulating previous run for example purposes\n",
    "fcm.fit(X_scaled)  # Another run (simulating for stability check)\n",
    "new_labels = fcm.u.argmax(axis=1)\n",
    "stability_ari = adjusted_rand_score(previous_labels, new_labels)\n",
    "\n",
    "# Output the results\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Davies-Bouldin Score: {db_index}\")\n",
    "print(f\"Calinski-Harabasz Score: {calinski_harabasz}\")\n",
    "print(f\"Average Shortest Euclidean Distance: {avg_shortest_distance}\")\n",
    "print(f\"Stability ARI: {stability_ari}\")\n",
    "print(\"Cluster Centers:\\n\", centers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e2e25",
   "metadata": {},
   "source": [
    "# Reporting the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster labels only to the rows from the prediction dataset\n",
    "df_prediction['Cluster'] = labels[prediction_start_index:]\n",
    "\n",
    "\n",
    "print(df_prediction)\n",
    "\n",
    "# Calculate summary statistics for each cluster - optional\n",
    "cluster_summary = df.groupby('Cluster').agg({\n",
    "    'spacing': ['mean', 'std', 'min', 'max'],\n",
    "    'interconnectivity': ['mean', 'std', 'min', 'max'],\n",
    "    'design': ['mean', 'std', 'min', 'max'],\n",
    "    'directness': ['mean', 'std', 'min', 'max'],\n",
    "    'service_area': ['mean', 'std', 'min', 'max'],\n",
    "    'Cluster': 'count'\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
